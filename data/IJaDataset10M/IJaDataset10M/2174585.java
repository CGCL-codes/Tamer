package net.sf.jtmt.indexers.nutch.indexer2;

import java.io.BufferedReader;
import java.io.File;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import org.apache.commons.codec.digest.DigestUtils;
import org.apache.commons.lang.StringUtils;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
import org.apache.hadoop.mapreduce.lib.reduce.LongSumReducer;
import org.apache.hadoop.util.GenericOptionsParser;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;
import org.apache.log4j.Logger;
import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.document.Document;
import org.apache.lucene.document.Field;
import org.apache.lucene.document.Field.Index;
import org.apache.lucene.document.Field.Store;
import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.IndexWriter;
import org.apache.lucene.index.IndexWriter.MaxFieldLength;
import org.apache.lucene.store.FSDirectory;
import org.apache.lucene.util.Version;
import org.apache.nutch.indexer.NutchDocument;
import org.apache.nutch.parse.ParseData;
import org.apache.nutch.parse.ParseText;
import org.apache.nutch.util.NutchConfiguration;

/**
 * Component to build a primitive "semantic" index, using a lexicon 
 * based on blog tags. Occurrences of a tag are counted and stored in
 * the index, along with their counts. Blog articles can then be looked
 * up by the tag and ranked using the tag counts.
 * 
 * The input to this index is the parse_data and parse_text information
 * in the crawled segments, the lexicon, and the index generated by the
 * default Nutch index mechanism.
 * 
 * The format of the lexicon is a flat file, with tags (single or multi
 * word) as the key, mapped to zero or more "semantic" meanings of the 
 * term. For example, when we notice a tag "cx_oracle", it is probably
 * not only about cx_oracle, but also about databases, the Oracle database,
 * the Python scripting language, and scripting in general. Hence, the 
 * entry for cx_oracle will look like this:
 * 
 * <pre>
 * cx_oracle:databases,oracle,scripting,python
 * ...
 * </pre>
 * 
 * Tags appearing in the title are given a 5x (currently, configured by
 * TITLE_BOOST in the code) boost, ie, each occurrence of a tag in a title
 * is counted 5 times, and each occurrence of a tag in the body is counted
 * once.
 * 
 * @author Sujit Pal
 * @version $Revision: 55 $
 */
public class Indexer2 extends Configured implements Tool {

    private static final Logger LOGGER = Logger.getLogger(Indexer2.class);

    private static final int TITLE_BOOST = 5;

    private static final int LABEL_CUTOFF_SCORE = 2;

    /**
   * Input: <url,ParseData> or <url,ParseText>
   * Output: List(<(url,term),count>)
   * Processing: extract text from ParseText and title from ParseData and
   * generate a list of terms in the lexicon that are found in these texts.
   * Send a List(<url,term>,count) to the reducer. The reducer is a (built-in)
   * LongSumReducer, which aggregates counts for each <url,term> key.
   * Configuration: the location of the lexicon file is passed in, which is
   * used to build up the lookup table that is used for the processing.
   */
    private static class Mapper1 extends Mapper<Text, Writable, Text, LongWritable> {

        private static Map<String, List<String>> DICTIONARY = null;

        private static Integer TITLE_BOOST = null;

        @Override
        public void setup(Context context) throws IOException, InterruptedException {
            if (DICTIONARY == null || TITLE_BOOST == null) {
                TITLE_BOOST = context.getConfiguration().getInt("title.boost", 0);
                DICTIONARY = new HashMap<String, List<String>>();
                String dictFile = context.getConfiguration().get("index2.dictfile");
                try {
                    FileSystem localFileSystem = FileSystem.getLocal(new Configuration());
                    Path dictPath = new Path(dictFile);
                    FSDataInputStream istream = localFileSystem.open(dictPath);
                    BufferedReader reader = new BufferedReader(new InputStreamReader(istream));
                    String line;
                    while ((line = reader.readLine()) != null) {
                        String[] nvp = StringUtils.split(line, ":");
                        List<String> values = new ArrayList<String>();
                        if (nvp.length > 1) {
                            String[] vals = StringUtils.split(nvp[1], ",");
                            for (String val : vals) {
                                values.add(val);
                            }
                        }
                        DICTIONARY.put(nvp[0], values);
                    }
                } catch (IOException e) {
                    LOGGER.error("Could not get DICTIONARY file: " + dictFile, e);
                    throw new RuntimeException(e);
                }
            }
        }

        @Override
        public void map(Text key, Writable value, Context context) throws IOException, InterruptedException {
            String url = key.toString();
            String text = null;
            long occurrenceBoost = 1;
            if (value instanceof ParseText) {
                ParseText parseText = (ParseText) value;
                text = parseText.getText();
            } else if (value instanceof ParseData) {
                ParseData parseData = (ParseData) value;
                text = parseData.getTitle();
                occurrenceBoost *= TITLE_BOOST;
            }
            if (text != null && (!StringUtils.trim(text).equals("null"))) {
                InvertedIndex invertedIndex = new InvertedIndex(text);
                for (String term : DICTIONARY.keySet()) {
                    int occurrences = invertedIndex.countOccurrencesOf(term);
                    if (occurrences > 0) {
                        String newKey = StringUtils.join(new String[] { url, term }, ",");
                        LongWritable score = new LongWritable(occurrences * occurrenceBoost);
                        context.write(new Text(newKey), score);
                        List<String> synonyms = DICTIONARY.get(term);
                        if (synonyms != null && synonyms.size() > 0) {
                            for (String synonym : synonyms) {
                                newKey = StringUtils.join(new String[] { url, synonym }, ",");
                                context.write(new Text(newKey), score);
                            }
                        }
                    }
                }
            }
        }
    }

    /**
   * Input: <(url,term),count>
   * Output: List(<url,(term:count)>
   * Processing: each record is re-parsed to be keyed by URL and passed to
   * the OutputCollector. Only terms with counts above a preconfigured cutoff
   * are collected. This is an attempt to remove label counts which have been
   * mentioned "in passing".
   */
    private static class Mapper2 extends Mapper<Text, LongWritable, Text, Text> {

        private static Float LABEL_CUTOFF_SCORE = null;

        @Override
        public void setup(Context context) {
            if (LABEL_CUTOFF_SCORE == null) {
                LABEL_CUTOFF_SCORE = context.getConfiguration().getFloat("label.cutoff.score", 0.0F);
            }
        }

        @Override
        public void map(Text key, LongWritable value, Context context) throws IOException, InterruptedException {
            String[] urlTermPair = StringUtils.split(key.toString(), ",");
            long count = value.get();
            if (count > LABEL_CUTOFF_SCORE) {
                context.write(new Text(urlTermPair[0]), new Text(StringUtils.join(new String[] { urlTermPair[1], String.valueOf(count) }, ":")));
            }
        }
    }

    /**
   * Input: <url,List(term:count)>
   * Output: List(<url,CSV(term:count)>)
   * Processing: flattens multiple terms and their associated aggregate counts
   * back to the same URL key.
   */
    private static class Reducer2 extends Reducer<Text, Text, Text, Text> {

        @Override
        public void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
            StringBuilder termCounts = new StringBuilder();
            int i = 0;
            for (Iterator<Text> it = values.iterator(); it.hasNext(); ) {
                Text value = it.next();
                if (i > 0) {
                    termCounts.append(",");
                }
                termCounts.append(value);
                i++;
            }
            context.write(key, new Text(termCounts.toString()));
        }
    }

    /**
   * Input: <url,CSV(term:count)>
   * Output:<digest(url,term),NutchDocument>
   * Processing: the url is used to lookup the record in the index built by
   * Nutch as part of its normal cycle. A NutchDocument is created for each
   * (url,term) combination with this information, and the corresponding term
   * and term count. A new unique key is generated for this output record using
   * an MD5 hash of the url and term.
   */
    private static class Mapper3 extends Mapper<Text, Text, Text, NutchDocument> {

        private static Map<String, Integer> URL_DOCID_MAP = null;

        private static IndexReader NUTCH_INDEX_READER = null;

        @Override
        public void setup(Context context) {
            try {
                if (URL_DOCID_MAP == null) {
                    URL_DOCID_MAP = new HashMap<String, Integer>();
                    NUTCH_INDEX_READER = IndexReader.open(FSDirectory.open(new File(context.getConfiguration().get("nutch.index.dir"))), true);
                    int numDocs = NUTCH_INDEX_READER.maxDoc();
                    for (int i = 0; i < numDocs; i++) {
                        Document doc = NUTCH_INDEX_READER.document(i);
                        String url = doc.get("url");
                        URL_DOCID_MAP.put(url, i);
                    }
                }
            } catch (Exception e) {
                LOGGER.error("Cannot open index reader on nutch index for lookup");
                throw new RuntimeException(e);
            }
        }

        @Override
        public void map(Text key, Text value, Context context) throws IOException, InterruptedException {
            Integer docId = URL_DOCID_MAP.get(key.toString());
            if (docId != null) {
                Document doc = NUTCH_INDEX_READER.document(docId);
                if (doc != null) {
                    String termCounts = value.toString();
                    String[] termCountPairs = StringUtils.split(termCounts, ",");
                    for (String termCountPair : termCountPairs) {
                        String[] components = StringUtils.split(termCountPair, ":");
                        NutchDocument nutchDoc = new NutchDocument();
                        String label = components[0];
                        String url = doc.get("url");
                        nutchDoc.add("label", label);
                        nutchDoc.add("label_count", components[1]);
                        nutchDoc.add("url", url);
                        nutchDoc.add("title", doc.get("title"));
                        String newKey = DigestUtils.md5Hex(StringUtils.join(new String[] { url, label }, ","));
                        context.write(new Text(newKey), nutchDoc);
                    }
                }
            }
        }

        @Override
        public void cleanup(Context context) throws IOException, InterruptedException {
            if (NUTCH_INDEX_READER != null) {
                try {
                    NUTCH_INDEX_READER.close();
                } catch (Exception e) {
                }
            }
        }
    }

    /**
   * Input: List(<digest(url,term),NutchDocument>)
   * Output: none
   * Processing: A new Lucene index is created (path to the index passed in).
   * For each NutchDocument, a corresponding record is written to the Lucene 
   * index.
   */
    private static class Reducer3 extends Reducer<Text, NutchDocument, Text, NutchDocument> {

        private static IndexWriter INDEX2_WRITER = null;

        @Override
        public void setup(Context context) {
            if (INDEX2_WRITER == null) {
                String indexOutputDir = context.getConfiguration().get("index2.output.dir");
                try {
                    INDEX2_WRITER = new IndexWriter(FSDirectory.open(new File(indexOutputDir)), new StandardAnalyzer(Version.LUCENE_30), MaxFieldLength.UNLIMITED);
                } catch (Exception e) {
                    LOGGER.error("Could not open index [" + indexOutputDir + "] for writing");
                    throw new RuntimeException(e);
                }
            }
        }

        @Override
        public void reduce(Text key, Iterable<NutchDocument> values, Context context) throws IOException, InterruptedException {
            for (Iterator<NutchDocument> it = values.iterator(); it.hasNext(); ) {
                NutchDocument nutchDoc = it.next();
                Document doc = new Document();
                doc.add(new Field("url", nutchDoc.getFieldValue("url"), Store.YES, Index.NOT_ANALYZED));
                doc.add(new Field("label", nutchDoc.getFieldValue("label"), Store.YES, Index.NOT_ANALYZED));
                doc.add(new Field("label_count", nutchDoc.getFieldValue("label_count"), Store.YES, Index.NOT_ANALYZED));
                doc.add(new Field("title", nutchDoc.getFieldValue("title"), Store.YES, Index.ANALYZED));
                INDEX2_WRITER.addDocument(doc);
            }
        }

        @Override
        public void cleanup(Context context) throws IOException, InterruptedException {
            if (INDEX2_WRITER != null) {
                try {
                    INDEX2_WRITER.optimize();
                    INDEX2_WRITER.close();
                } catch (Exception e) {
                }
            }
        }
    }

    private void analyze(Configuration conf, Path indexDir, String dictFile, List<Path> segments, int titleBoost) throws Exception {
        LOGGER.info("Stage 1 (analyze)");
        Job job1 = new Job(conf, "index2-analyze");
        job1.getConfiguration().set("index2.dictfile", dictFile);
        job1.getConfiguration().set("title.boost", String.valueOf(titleBoost));
        for (Path segment : segments) {
            FileInputFormat.addInputPath(job1, new Path(segment, ParseData.DIR_NAME));
            FileInputFormat.addInputPath(job1, new Path(segment, ParseText.DIR_NAME));
        }
        FileOutputFormat.setOutputPath(job1, new Path(indexDir, "stage1"));
        job1.setMapperClass(Mapper1.class);
        job1.setReducerClass(LongSumReducer.class);
        job1.setInputFormatClass(SequenceFileInputFormat.class);
        job1.setOutputFormatClass(SequenceFileOutputFormat.class);
        job1.setMapOutputKeyClass(Text.class);
        job1.setMapOutputValueClass(LongWritable.class);
        job1.setOutputKeyClass(Text.class);
        job1.setOutputValueClass(LongWritable.class);
        boolean status = job1.waitForCompletion(true);
        if (!status) {
            throw new Exception("Job " + job1.getJobName() + " failed!");
        }
    }

    private void flatten(Configuration conf, Path indexDir, int labelCutoffScore) throws Exception {
        LOGGER.info("Stage 2 (flatten)");
        Job job2 = new Job(conf, "index2-normalize");
        job2.getConfiguration().set("label.cutoff.score", String.valueOf(labelCutoffScore));
        FileInputFormat.addInputPath(job2, new Path(indexDir, "stage1"));
        FileOutputFormat.setOutputPath(job2, new Path(indexDir, "stage2"));
        job2.setMapperClass(Mapper2.class);
        job2.setReducerClass(Reducer2.class);
        job2.setInputFormatClass(SequenceFileInputFormat.class);
        job2.setOutputFormatClass(SequenceFileOutputFormat.class);
        job2.setMapOutputKeyClass(Text.class);
        job2.setMapOutputValueClass(Text.class);
        job2.setOutputKeyClass(Text.class);
        job2.setOutputValueClass(Text.class);
        boolean status = job2.waitForCompletion(true);
        if (!status) {
            throw new Exception("Job " + job2.getJobName() + " failed!");
        }
    }

    private void merge(Configuration conf, Path indexDir, String nutchIndexDir) throws Exception {
        LOGGER.info("Stage 3 (merge)");
        Job job3 = new Job(conf, "index2-merge");
        job3.getConfiguration().set("nutch.index.dir", nutchIndexDir);
        job3.getConfiguration().set("index2.output.dir", new Path(indexDir.getParent(), "index2").toString());
        job3.setJobName("index2-merge");
        FileInputFormat.addInputPath(job3, new Path(indexDir, "stage2"));
        FileOutputFormat.setOutputPath(job3, new Path(indexDir, "stage3"));
        job3.setMapperClass(Mapper3.class);
        job3.setReducerClass(Reducer3.class);
        job3.setInputFormatClass(SequenceFileInputFormat.class);
        job3.setOutputFormatClass(SequenceFileOutputFormat.class);
        job3.setMapOutputKeyClass(Text.class);
        job3.setMapOutputValueClass(NutchDocument.class);
        job3.setOutputKeyClass(Text.class);
        job3.setOutputValueClass(NutchDocument.class);
        boolean status = job3.waitForCompletion(true);
        if (!status) {
            throw new Exception("Job " + job3.getJobName() + " failed!");
        }
    }

    public int run(String[] args) throws Exception {
        Configuration conf = new Configuration();
        String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
        if (otherArgs.length < 4) {
            System.err.println("Usage: Indexer2 <index> <dictfile> <nutch_index_dir> <segment> ...");
            return -1;
        }
        Path indexDir = new Path(args[0]);
        String dictFile = args[1];
        String nutchIndexDir = args[2];
        final List<Path> segments = new ArrayList<Path>();
        for (int i = 3; i < args.length; i++) {
            segments.add(new Path(args[i]));
        }
        try {
            LOGGER.info("Starting index2");
            analyze(conf, indexDir, dictFile, segments, TITLE_BOOST);
            flatten(conf, indexDir, LABEL_CUTOFF_SCORE);
            merge(conf, indexDir, nutchIndexDir);
            LOGGER.info("Indexer2: done");
        } catch (Exception e) {
            LOGGER.fatal(e);
            return -1;
        }
        return 0;
    }

    public static void main(String[] args) throws Exception {
        int result = ToolRunner.run(NutchConfiguration.create(), new Indexer2(), args);
        System.exit(result);
    }
}
